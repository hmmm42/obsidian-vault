[直播弹幕系统设计 | 王帅真’s Blog](https://blog.qizong007.top/article/live-streaming-bullet-system)
## 特点
> 其实很类似IM即时通讯系统，是个变种，本质也是在一个空间内收发消息
- 消息及时性强，过期消息意义不大
- 用户松散，随时来随时走
- 可能有瞬时大批量弹幕（比如比赛精彩部分）
- 流量特点：读多写少
## 弹幕数据结构
```go
type Bullet struct {
	UserId    int // 用户ID
	Content   string // 内容
	Timestamp int // 弹幕发送时间
	Extra     *Extra // 效果、样式
}
```
> 视频弹幕才有偏移时间，直播应该不用（因果一致性）
## MVP版本
### 整体设计
> 读写分离架构，短连接+拉模式
- 写：若不考虑历史弹幕可回放，可以直接使用 Redis 作为唯一存储
- 读：Redis 主要用于读缓存，缓存直播间最新的弹幕数据
![notion image](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa44b4fa3-3355-473e-911f-048582808856%2Ff8d17bf9-0ac7-4c5c-9906-211e849c268a%2FUntitled.png?table=block&id=0cd1e39e-a701-43bc-9997-0f57fcc7bc95&t=0cd1e39e-a701-43bc-9997-0f57fcc7bc95&width=576&cache=v2)
### 存储
数据结构选择**Redis的ZSet**
- 需要弹幕按时间排序，而ZSet可保证有序
- 此外，score允许重复，那我们就用timestamp来做score
### 使用
- 发弹幕：`ZADD 直播间ID, 弹幕val, timestamp`（优化：时间只存相对于某个时间点的delta）
- 拉取弹幕：`ZRangeByScore` 定时轮询（秒级，准实时即可）
### 问题
1. 弹幕怎么持久化（Redis扩容成本高）
2. 热门直播会有大量瞬时弹幕，挑战Redis并发瓶颈
3. Redis重复请求多，相同直播间会存在很多重复的轮询请求
#### 问题cover
1. 持久化
	1. 若考虑支持弹幕的回放，数据还是需要持久化，可以考虑使用 MySQL
		1. 弹幕在直播结束后迁移至 DB（可以专门拿一台从redis做回放）
		2. 异步刷入（开线程/监听redis日志刷/发MQ消费）
	2. 如果有更高性能的写需求，可以考虑NoSQL DB，如：HBase、OpenTSDB 等
	3. 新问题：Write-Behind 这种以cache为主的模式，是可能会丢数据的
		1. DB挂了，少写数据，但是只要数据还在redis能追回来
		2. redis挂了，服务感知到后降级，请求回源，“[缓存击穿](https://xiaolincoding.com/redis/cluster/cache_problem.html#%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF)”
2. 使用MQ来消化瞬时弹幕，削峰；弹幕返回条数根据直播间的大小自动调整（热门直播间对时间跨度以及消息条数做更严格的限制）
3. 对弹幕读请求，使用local cache缓存最近5s的数据在应用服务的内存中（过期了才回源redis）
	1. 新问题：如果直播间变多，本地内存使用量随直播间线性膨胀，本地cache的命中率下降，还可能频繁触发GC
	2. 解决
		1. 只针对热门直播间使用本地cache
		2. 使用“一致性hash”，控制同一直播间尽可能打到同一台服务器，降低本地cache使用量
#### 优化：热门直播间
- 需要对每个直播间进行指标采样
- 标准：粉丝数、在线粉丝数、是否有活动
- 弹幕系统需要与直播间系统隔离
- 请求时带上直播间的“热门”标识
- 根据服务器机器资源来分配所承载的热门直播间
#### 长连接轮询
此前短连接的方式，每次轮询请求后都要重新建立连接。如果使用长连接轮询，客户端保持连接处于打开状态，就不会一直重新建立链接，减少重复的资源消耗，缩短响应时间。
不过，长连接轮询还是有一些缺点的：
- 发送者和接收者可能并没有连接到同一个服务器
- HTTP协议的服务器通常是无状态的，如果使用Round Robin的方式来做负载均衡，接收到弹幕的服务器可能并没有与等待接收消息的客户端保持长轮询连接
- 服务器没有好的方法来判断客户端有没有断开连接
### 架构图
![notion image](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa44b4fa3-3355-473e-911f-048582808856%2F76474fa7-0b7f-4804-aa30-dc91787b5919%2FUntitled.png?table=block&id=9232f0b8-327f-4e7e-a92f-9c431affec35&t=9232f0b8-327f-4e7e-a92f-9c431affec35&width=1742&cache=v2)
## 推模式
> 上面MVP版本是拉模式，适合前期；中后期规模和性能要求上来了，需要引入推模式。
### 架构图
![notion image](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa44b4fa3-3355-473e-911f-048582808856%2Fce315253-c9c2-46bc-9415-ed95f57d07f1%2FUntitled.png?table=block&id=ce3517b5-2756-43ff-a8f5-4f046671313d&t=ce3517b5-2756-43ff-a8f5-4f046671313d&width=1778&cache=v2)
### 长连接推送
为了保障客户端消息的推送性能和实时性，需要引入长连接
- Push Server：负责推送（不感知直播业务）
- 存储：从 Redis 中获取用户和直播间的关系以及长连接信息
- 推送：分批并发推
- connection proxy：只负责与客户端保持长连接
### 使用技术
- WebSocket（HTTP 2.0支持的全双工通信+长连接）
- Session（基于redis，通过用户会话来推）
### 优化
- 有状态服务需要做好路由
- 两个批量推送：消费 MQ 后批量推；从 push server 到连接代理的每个连接上也批量
- 推拉结合，可以把短连接的拉作为长连接推的降级方案（服务端监控+客户端上报）
### 其他挑战
- 写扩散（m条消息*n次扩散，平方级别）⇒ MQ控制推送速率
- 丢弹幕 ⇒ push 失败引入重试；落 DB 引入 ACK
- 带宽压力
- 灰度拉切推
## 其他思路
- 使用 redis 的 [Stream](https://www.runoob.com/redis/redis-stream.html) 来做弹幕的发送（生产）和阅读（消费）
- 前端展示上的优化
	- 弹幕去重
	- 发送弹幕后，本地优先展示（即使它丢了 or delay）

### 1\. 系统特点与需求分析 (System Characteristics & Requirements)

在设计之初, 首先需要明确弹幕系统的核心特点:

  * **高实时性:** 弹幕需要被尽快地展示给房间内的其他用户, 延迟过高会失去意义.
  * **用户松散:** 用户随时加入、随时离开直播间.
  * **瞬时高并发:** 在直播热门时段 (如电竞比赛精彩时刻), 弹幕量会瞬间暴增.
  * **本质:** 类似于一个多人群聊的即时通讯 (IM) 系统, 但有其特殊性.

**核心数据结构 (抽象):**

```
struct Danmaku {
    string user_id;      // 发送者ID
    string content;      // 弹幕内容
    int64  timestamp;    // 发送时间戳 (服务器时间)
    map    extra_data;   // 扩展信息 (如样式, 效果等)
}
```

-----

### 2\. 架构演进路径 (Architectural Evolution Path)

分享者采用了架构演进的方式来阐述设计, 这是系统设计题的一个优秀思路.

#### 阶段一: 基础MVP方案 (短连接 + 拉模式)

这个阶段的目标是快速实现核心功能, 暂时不考虑持久化和高并发问题.

  * **架构:** 客户端使用 **HTTP短连接**, 定时轮询 (Pull) 后端接口来获取新弹幕.

  * **存储:** 直接使用 **Redis** 作为唯一存储. 因为直播弹幕的即时性很强, 如果不考虑回放, 直播结束后数据丢失是可以接受的.

  * **技术实现:**

      * **数据结构:** 使用 Redis 的 `ZSET`.
      * **Key:** `room_id` (直播间ID).
      * **Score:** `timestamp` (弹幕发送时间戳), 用于排序.
      * **Value:** 弹幕内容的序列化字符串 (如JSON).
      * **写弹幕:** `ZADD room_id timestamp danmaku_content`.
      * **读弹幕:** `ZRANGEBYSCORE room_id last_timestamp +inf`, 客户端记录上次拉取到的最新时间戳, 每次只拉取增量数据.

  * **此阶段的问题:**

    1.  **无持久化:** 数据在 Redis 中, 存在丢失风险, 无法支持回放.
    2.  **Redis瓶颈:** 热点直播间的所有读写压力都集中在单个 Redis Key 上, 容易达到瓶颈.
    3.  **重复请求:** 大量客户端拉取同一个时间窗口的数据, 存在大量重复的无效请求.
    4.  **连接开销:** 频繁的短连接建立和销毁有性能损耗.

-----

#### 阶段二: 拉模式的优化与完善

针对第一阶段的问题, 逐一进行优化.

  * **1. 解决持久化问题 (Persistence):**

      * **方案:** 引入数据库 (如MySQL, HBase), 采用 **Cache-Aside / Write-Behind** 模式. 写操作先写 Redis, 然后异步地将数据刷入DB.
      * **异步方式:**
          * 在应用层通过独立线程/协程写入DB.
          * 监听 Redis 的 `binlog` (RDB/AOF) 进行同步.
          * 发送消息到 **MQ (消息队列)**, 由一个专门的消费者服务负责写入DB. (推荐)
      * **故障处理:**
          * **DB宕机:** 数据暂存在Redis, DB恢复后可从Redis追回数据.
          * **Redis宕机:** 服务降级, 流量直接打到DB. 但这会引发 **缓存击穿** 问题, 需要有相应的保护策略 (如限流, 熔断).

  * **2. 解决热点直播间问题 (High Traffic):**

      * **写压力 (削峰):** 在写入 Redis 前增加一层 **MQ**. 用户发弹幕的请求先快速写入MQ, 后端服务再平稳地从MQ消费并写入Redis.
      * **读压力 (限流/业务优化):**
          * **动态调整返回量:** 根据直播间热度, 动态调整接口一次返回的弹幕数量和时间窗口大小. 热度越高, 返回的弹幕越少 (例如, 只返回最新的30条).
          * **系统隔离:** 弹幕系统和直播间系统做好隔离, 不要强耦合.

  * **3. 解决重复请求问题 (Redundant Requests):**

      * **方案:** 引入 **本地缓存 (Local Cache)**. 在应用服务的内存中缓存热点直播间最近几秒的数据.
      * **问题:** 本地缓存会随直播间数量线性膨胀, 占用内存, 引发GC.
      * **优化:**
          * **只缓存热点房间:** 通过指标采样 (如在线人数, 粉丝数) 识别热点房间, 只对这些房间开启本地缓存.
          * **一致性哈希:** 采用一致性哈希策略进行负载均衡, 使得同一个直播间的请求尽可能落到同一台服务器上, 提高本地缓存命中率.

  * **4. 解决连接效率问题 (Connection Efficiency):**

      * **方案:** 从 **HTTP短连接** 优化为 **HTTP长连接轮询 (Long-Polling)**.
      * **优势:** 减少了TCP连接频繁建立和销毁的开销.
      * **问题:** 对服务端状态维护有要求, 负载均衡策略需要考虑会话保持 (Session Affinity).

-----

#### 阶段三: 引入推模式 (Push Model)

当系统规模和性能要求进一步提高时, "拉模式" 的延迟和资源浪费问题凸显, 此时引入 "推模式".

  * **架构:**
      * **客户端:** 与服务器建立 **长连接** (如 **WebSocket**).
      * **新增组件:**
        1.  **连接网关 (Connection Gateway):** 专门负责维护与客户端的长连接, 管理连接状态.
        2.  **推送服务 (Push Server):** 负责业务逻辑, 从MQ获取新弹幕后, 查询哪些用户需要被推送, 并将消息发给连接网关.
  * **数据流:**
    1.  用户发弹幕 -\> MQ.
    2.  消费者服务消费MQ -\> (1) 写入 Redis (用于持久化和历史消息) (2) 将消息发给 Push Server.
    3.  Push Server 从 Redis 或其他地方获取该直播间的订阅关系 (哪些用户在线).
    4.  Push Server 将弹幕消息批量推送到对应的连接网关.
    5.  连接网关通过长连接将弹幕下发给客户端.
  * **优化:** 推送过程采用 **批量 (Batching)** 和 **并发 (Concurrency)** 策略来提高性能.

-----

#### 阶段四: 推拉结合的混合模式 (Hybrid Push-Pull Model)

这是最成熟和鲁棒的方案.

  * **核心思想:** 以 **推模式为主**, **拉模式为辅**.
  * **应用场景:**
      * **降级方案:** 当长连接推送失败 (如网络问题, 客户端断连) 时, 客户端可以自动降级为短连接轮询模式来获取弹幕, 保证用户体验.
      * **消息补齐:** 客户端重连后, 可以通过拉模式获取断连期间错过的历史弹幕.

-----

### 4\. 其他挑战与优化点

  * **写扩散 (Fan-out):** 一条弹幕消息需要扩散给成千上万的用户, 这是一个 M (消息) \* N (用户) 的问题, 对推送系统压力巨大. 可通过MQ控制推送速率.
  * **消息必达:** 如果要求不丢弹幕, 需要引入 ACK 确认机制和重试策略.
  * **带宽压力:** 大量弹幕会对服务器和客户端带宽造成压力.
  * **灰度迁移:** 如何从一个已有的拉模式架构平滑迁移到推模式架构, 是一个工程挑战.
  * **前端优化:**
      * **本地优先展示:** 用户自己发送的弹幕, 不用等服务器返回, 立即在本地渲染出来 (乐观UI).
      * **弹幕去重:** 网络原因可能导致收到重复弹幕, 前端需要做去重.