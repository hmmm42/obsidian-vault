# RPC
> 桩文件 → 序列化 → TCP
- 定义 IDL 文件，编译工具生成 stub 桩文件，相当于生成了静态库，实现函数映射
- 网络里传输的数据都是二进制数据，需要把请求参数、返回结果进行 encode 和 decode
- 根据 RPC 协议 约定数据头、元数据、消息体等，保证有 ID 能使请求和返回结果做到一一映射。
- 基于成熟的网络库进行 TCP / UDP 传输
## 面试题：描述 RPC 的通信流程
- 函数映射：静态代理，生成 stub 文件
	- 对比建立 HTTP 请求连接，RPC 在编写代码时，降低了复杂度
	- stub 文件让远程调用看上去像是本地调用
- 序列化：为了生成二进制数据
	- HTTP/1 直接发送 JSON，明文传输
	- gRPC 以 protobuf 作为序列化协议
- 网络传输
	- 自定义 RPC 协议实现通信，大厂几乎都用自定义 RPC 框架去自定义 RPC 协议。
	- 使用成熟的网络库，实现多路复用、可靠传输。
## 比较 HTTP / RPC
- HTTP 是应用层协议。
- RPC 是远程过程调用，它是调用方式，对应的是本地调用。
- 所谓的 RPC 协议，实际上是基于 TCP、UDP、甚至 HTTP2 改造后的自定义协议。
### 编（解）码层
> 网络传输前，需要结构体转为二进制数据 → 序列化
- HTTP/1.1
	- 额外空间开销大，没有类型，开发时需要通过反射统一解决。
	- 序列化协议：JSON
![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/f606K72wESCsVr5dPGbBe3Xso7p2XGxK48cNoIDdwibd9XhJqYP6tlKibkRXyeooWYTcziaDIkApAM14nAkRTsF4g/640?wx_fmt=png&randomid=ole5f93f&tp=webp&wxfrom=5&wx_lazy=1)
- RPC
	- 序列化后的体积比 JSON 小 ⇒ 传输效率高
	- 序列化、反序列化速度快，开发时不需要通过反射 ⇒ 性能消耗低
	- IDL 描述语义比较清晰。
	- 序列化协议：以 gRPC 为代表的 Protobuf，其他也类似
### 通信协议约定
> 基于 TCP 传输，都会有消息头和消息体，区别在于消息头
- HTTP/1.1
	- 优点是灵活，可以自定义很多字段。
	- 缺点是包含许多为了适应浏览器的冗余字段，这些是内部服务用不到的。
- RPC
	- 可定制化，自定义必要字段即可。
	- 可摒弃很多 HTTP Header 中的字段，比如各种浏览器行为。
### 网络传输层
> 本质都是基于 Socket 通信
- HTTP/1.1
	- 建立一个 TCP 长连接，设置 keep-alive 长时间复用这个连接。
	- 框架中会引入成熟的网络库，给 HTTP 加连接池，保证不只有一个 TCP 连接可用。
- RPC
	- 建立 TCP 连接池，框架也会引入成熟网络库来提高传输性能。
	- gRPC 基于 HTTP/2，拥有多路复用、优先级控制、头部压缩等优势。
![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/f606K72wESCsVr5dPGbBe3Xso7p2XGxKVZichWGibxABzWAev2sicFl9fhaaSic1pE0cUoKzCAkSAsDUFrLicpNmU8A/640?wx_fmt=png&randomid=nre8ubv1&tp=webp&wxfrom=5&wx_lazy=1)
## RPC 的优势和不足
### 优势
- 相较于 HTTP/1.1，数据包更小、序列化更快，所以传输效率很高。
- 基于 TCP 或 HTTP/2 的自定义 RPC 协议，网络传输性能比 HTTP/1.1 更快。
- 适用于微服务架构，微服务集群下，每个微服务职责单一，有利于多团队的分工协作。
### 不足
- RPC 协议本身无法解决微服务集群的问题，例如：服务发现、服务治理等，需要工具来保障服务的稳定性。
- 调用方对服务端的 RPC 接口有强依赖关系，需要有自动化工具、版本管理工具来保证代码级别的强依赖关系。例如，stub 桩文件需要频繁更新，否则接口调用方式可能出错。
## RPC 框架
### 编解码层
#### 目标
- 生成代码：代码生成工具将 IDL 文件转换成不同语言可以依赖的 lib 代码（类似于库函数）
- 序列化 & 反序列化：对象 ↔ 二进制字节流
#### 选型
- 安全性
	- 通用性：跨语言、跨平台
	- 兼容性：序列化协议升级后，保证原服务的稳定性
- 性能
	- 时间：序列化反序列化的速度
	- 空间：序列化后的数据体积大小，体积越小，网络传输耗时越短
### 协议层
#### 目标
- 支持解析多种协议，包含 HTTP、HTTP2、自定义 RPC 协议、私有协议等。
#### RPC 通信协议的设计
> 大厂内部大部分用自定义的 RPC 协议，灵活 + 安全
- 作用：TCP 通道中的二进制数据包，会被拆分、合并，需要应用层协议确定消息的边界（说人话：得知道哪几个二进制包是这一条请求）
- 协议构成
	- 协议头 - 固定部分：整体长度、协议头长度、消息类型、序列化方式、消息 ID 等
	- 协议头 - 扩展部分：不固定的扩展字段，各种协议 DIY 的字段
	- 协议体：业务数据
![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/f606K72wESAo9XwMYUNFDFzQX2ibx1wyDnNos4rsp1n2ocFoguGTCcu9lkoXw2YBWeCN5slPhDytqtnRrDtV8nA/640?wx_fmt=png&randomid=is5xdkrk&tp=webp&wxfrom=5&wx_lazy=1)
### 网络传输层
> 一般使用成熟的网络通信框架（例如：Netty），会和 RPC 框架解耦
#### 目标
- IO 多路复用实现高并发，可靠传输
#### 选型指标
- 易用：封装原生 socket API
- 性能：零拷贝、建立连接池、减少 GC 等
## RPC 热门框架
### 跨语言调用型
- 典型代表：grpc，thrift
- 特点：
	- 提供最基础的 RPC 通信能力。
	- 专注于跨语言调用，适合不同语言提供服务的场景。
	- 没有服务治理等相关机制，需要借助其他开源工具去实现服务发现、负载均衡、熔断限流等功能。
### 服务治理型
- 典型代表：rpcx，kitex，dubbo
- 特点：
	- 服务定义（函数映射）
	- 多消息传输协议（序列化协议）
	- 多网络通信协议（TCP、UDP、HTTP/2、QUIC 等）
	- 提供最基础的 RPC 通信能力。
	- 提供服务治理能力：服务发现、负载均衡、熔断限流等。
### 框架功能简单介绍
#### 分层设计
- 调用层：封装服务，提供RPC调用接口
- 服务治理层：服务发现、负载均衡、熔断限流等
- 通信层：多网络通信协议、多消息传输协议（编解码、序列化、压缩）
![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/f606K72wESAo9XwMYUNFDFzQX2ibx1wyDALg6AlPLRvn3ceMDSuPBAvOcib8libCHHNGbZyvABu5icKIwhMPfC5Njw/640?wx_fmt=png&randomid=phit5j9s&tp=webp&wxfrom=5&wx_lazy=1)
#### 服务治理层
![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/f606K72wESAo9XwMYUNFDFzQX2ibx1wyDDG0o5KU2G3bDTI2DuRUIAg8AJibNa1Q7ru0eWqMxztMmT2Dq509H6Ug/640?wx_fmt=png&randomid=2h8ll6mr&tp=webp&wxfrom=5&wx_lazy=1)
- 服务端
	- 服务注册：上报服务名和服务的 IP、端口
	- 健康检测：第一时间让调用方知道服务出现问题
	- 限流：过载保护，访问量过大，抛出限流异常
- 客户端
	- 服务发现：根据服务名发现服务的 IP、端口
	- 路由策略：实现流量隔离，应用于灰度发布、隔离联调环境
	- 负载均衡：把请求分发到服务集群的每个服务节点
	- 重试机制：捕获异常，根据负载均衡再次选择节点重发请求
	- 故障熔断：确定下游异常，请求直接被截断，快速执行失败
#### 其他基础功能
- 日志 Log
- 监控 Metric
- 链路追踪 Tracing
- 丰富的插件机制
# 服务注册 服务发现
让 Client 端根据服务名找到 Server 的地址
不用 DNS:
- 多级缓存, 不能及时感知节点变化
- 不能注册端口, 只能注册 HTTP
**引入 Registry**
## 服务上线
- Server 启动后，向 Registry 注册自身信息
	- Registry 保存着所有服务的节点信息
	- Server 与 Registry 保持心跳，Registry 需要感知 Server 是否可用
- Client **第一次**发起 RPC 调用前，向 Registry 请求服务节点列表，并把这个列表缓存在本地
	- Client 与 Registry 保持数据同步，服务节点有变化时，Registry 通知 Client，Client 更新本地缓存
- Client 发起 RPC 请求，Server 返回响应
## 服务中心选型: CAP理论
- CP: 牺牲可用性, 保证一致性 Zookeeper, etcd
- AP: 牺牲一致性, 保证可用性 Eureka, Nacos
选型:
- 体量小时选CP
- 体量大选AP
	- 有大批服务节点同时上下线, 负载过高
	- 同步大量数据, 服务长时间不可用
## 识别服务节点可用情况: 心跳机制
[[融合点评+电商#consul节点故障]]
- Server 每隔几秒向 Registry 发送心跳包，收到响应则表示服务节点正常，在指定时间内没收到响应，则判定为失败
- 注册中心发现某个节点不可用时，会通知 Client，Client 更新本地缓存的服务节点列表
- 发现心跳断了，Registry 立即通知 Client 某节点不可用，避免服务真的宕机时，仍然有请求发来
- Registry 继续向 Server 发心跳，如果发几次心跳都是失败的，才认为服务节点不可用。如果心跳恢复，再告知 Client 服务节点可用
	- 重试策略：先连续发几次心跳，过一定时间间隔后再发心跳，需要考虑重试次数和重试间隔
# 负载均衡
属于客户端的工作
- 将请求均匀发给每个节点
- 优先发给相应最快的节点
## 分类
### 随机, 加权随机
请求量大, 各节点性能差异不大
### 轮询, 加权轮询
- 按固定顺序逐个访问
- 访问成功的节点权重增加
适合存在新老机器, 节点性能不同
### 哈希, 一致性哈希
哈希环
与本地缓存结合, 同一来源的请求映射到同一节点
适合不同客户端请求差异大, 需要用到本地缓存 / 节点增加, 减少
## 指标
- 最少连接
- 最少活跃
	- 已经接收但是还没返回
- 最快响应时间 
# 熔断
场景: 服务端出现问题
- 服务指标: 响应时间, 错误率, 连续错误数
- 硬件指标: CPU, 内层, 网络IO
目的:
- 为服务端恢复争取时间
- 避免全调用链路崩溃 (其他服务也堆积)
## 流程
1. Server 被监控到异常，触发熔断，熔断器抛出熔断的异常响应
2. Client 收到异常，利用负载均衡重新选择节点，后续请求不再打到被熔断的节点一段时间后，Client 再对这个节点重新请求，如果正常响应，则缓慢对这个节点放开流量，如果3.仍然是熔断，则继续执行 Step 2，如此循环
# 限流
## 静态算法
### 令牌桶
系统以恒定速率产生令牌并把令牌放到桶里，每个请求从桶里拿到令牌才会被执行，反
之被限流
### 漏桶
产生的令牌没被取走也不会积攒下来, **无法应对偶发性流量突变** 
### 固定窗口
固定时间段内只处理固定数量的请求
### 滑动窗口
随时间移动窗口
## 动态算法: BBR
### 原理
BBR 的名字已经揭示了它的两大核心指标：
1. **瓶颈带宽 (Bottleneck Bandwidth, `BtlBw`)**: 指的是一条网络链路中，吞吐量最低的那一段的带宽。这是决定你传输速率的“天花板”。
2. **往返传播时间 (Round-trip propagation time, `RTprop`)**: 指的是一个数据包从发送端到接收端，再回到发送端所需的最短时间（不包括在路由器中排队等待的时间）。

BBR 的目标就是找到一个理想的平衡点：**发送速率恰好等于瓶颈带宽，同时在途的数据量（inflight data）刚好填满整个网络管道（`BDP = BtlBw × RTprop`）**。这样既能跑满带宽，又不会在路由器中产生额外的排队延迟。
为了实现这个目标，BBR 的工作模式就像一个智能的探测机器人，它会周期性地进行以下探测：

-  **探测带宽 (`ProbeBW`)**: BBR 会在大部分时间里以它估计的瓶颈带宽速率发送数据。但为了确认带宽是否发生了变化（比如网络变好了），它会周期性地、短暂地**提高发送速率**（比如提高25%）。
    - 如果此时发现数据传输速率也跟着提升了，BBR 就知道瓶颈带宽增大了，于是更新 `BtlBw` 的估计值。
    - 如果速率没有提升，说明已经触及天花板了，多发的数据开始在路由器排队。
- **排空队列 (`Drain`)**: 在短暂提高了发送速率后，BBR 会立即进入一个“排空”阶段。它会短暂地**降低发送速率**（比如降低到之前速率的75%），把刚才因超发而积压在路由器队列中的数据包“排干”，从而降低延迟。
- **探测延迟 (`ProbeRTT`)**: 为了获得最准确的往返传播时间 `RTprop`，BBR 会周期性地进入一个探测延迟的状态。它会把在途数据量降到一个非常小的水平（例如只有4个数据包），持续一小段时间（如200毫秒）。这时测得的往返时间就可以认为是几乎没有排队延迟的 `RTprop`
通过在这几个状态之间不断地循环，BBR 能够动态、实时地跟踪网络瓶颈带宽和最小延迟的变化，从而调整出一个最优的发送速率。
### BBR 的优势
1. **高吞吐和低延迟**：在大多数网络环境下，尤其是在“长肥网络”（高带宽、高延迟）中，BBR 能在跑满带宽的同时，保持非常低的延迟。
2. **丢包不敏感**：它不依赖丢包来判断拥塞，因此在有少量随机丢包的网络中表现远超传统算法，不会轻易降速。
3. **快速启动**：启动时能更快地探测到链路的可用带宽，迅速达到高发送速率
## 流程
- 在中间件记录流量和阈值，并在中间件中实现限流算法。
- 对于偶发性的触发限流，只要在超时范围内 ，可以同步阻塞等待请求被处理。
- Server 的某个节点触发了非偶发性限流, Client 利用负载均衡调低该节点的权重，尽量少向这个节点发请求。
	- 区别于熔断的不再发请求，限流仍然会发请求，只是降低频率
# 降级
## 场景 & 目的
系统出现故障后的补救措施，或可预见的故障前的应对措施，来保证整体的可用性，
## 手段
- 考虑停用部分监控埋点、日志上报等观测类中间件
- 根据业务场景判断，停用边缘服务，返回服务繁忙之类的响应。
- 对于有缓存的接口，降级时只查缓存，不查 DB，没命中缓存则返回错误的响应
## 核心思想
- 如何判断节点的健康状态?是否需要熔断/限流/降级?
	- 通过监控看指标:QPS、连接数、节点负载等。
- 熔断/限流/降级后，怎么恢复?
	- 熔断限流搭配负载均衡，等节点恢复正常后，再重新选择。
	- 降级有时是手动恢复。