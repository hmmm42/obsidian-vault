# 性质
- 独占性：对于同一把锁，在同一时刻只能被一个取锁方占有，这是锁最基础的一项特征
- 健壮性：即不能产生死锁（dead lock）. 假如某个占有锁的使用方因为宕机而无法主动执行解锁动作，锁也应该能够被正常传承下去，被其他使用方所延续使用
- 对称性：加锁和解锁的使用方必须为同一身份. 不允许非法释放他人持有的分布式锁
- 高可用：当提供分布式锁服务的基础组件中存在少量节点发生故障时，应该不能影响到分布式锁服务的稳定性
# 思路
Redis 实现分布式锁, 思路是 **主动轮询**
- 针对于同一把分布式锁，使用同一条数据进行标识（以 redis 为例，则为同一个 key 对应的 kv 数据记录）
- 假如在存储介质成功插入了该条数据（要求之前该 key 对应的数据不存在），则被认定为加锁成功
- 把从存储介质中删除该条数据这一行为理解为释放锁操作
- 倘若在插入该条数据时，发现数据已经存在（锁已被他人持有），则持续轮询，直到数据被他人删除（他人释放锁），并由自身完成数据插入动作为止（取锁成功）
- 由于是并发场景，需要保证【（1）检查数据是否已被插入（2）数据不存在则插入数据】这两个步骤之间是原子化不可拆分的（在 redis 中是 set only if not exist —— `SETNX`操作）
# 加锁
方式分为`tryLock`和`Lock`, 根据是否处于阻塞模式来区分
```go
const RedisLockKeyPrefix = "REDIS_LOCK_PREFIX_"

// Lock 加锁.
func (r *RedisLock) Lock(ctx context.Context) (err error) {
	defer func() {
		if err != nil {
			return
		}
		// 加锁成功的情况下，会启动看门狗
		// 关于该锁本身是不可重入的，所以不会出现同一把锁下看门狗重复启动的情况
		r.watchDog(ctx)
	}()
    
	// 不管是不是阻塞模式，都要先获取一次锁
	err = r.tryLock(ctx)
	if err == nil {
		return nil
	}
	
	// 非阻塞模式加锁失败直接返回错误
	if !r.isBlock {
		return err
	}
	
	// 判断错误是否可以允许重试，不可允许的类型则直接返回错误
	if !IsRetryableErr(err) {
		return err
	}
	
	// 基于阻塞模式持续轮询取锁
	return r.blockingLock(ctx)
}

func (r *RedisLock) tryLock(ctx context.Context) error {
	// 首先查询锁是否属于自己
	reply, err := r.client.SetNEX(ctx, r.getLockKey(), r.token, r.expireSeconds)
	if err != nil {
		return err
	}
	if reply != 1 {
		return fmt.Errorf("reply: %d, err: %w", reply, ErrLockAcquiredByOthers)
	}
	return nil
}

func (r *RedisLock) getLockKey() string {
	return RedisLockKeyPrefix + r.key
}
```

```go
func (r *RedisLock) blockingLock(ctx context.Context) error {
	// 阻塞模式等锁时间上限
	timeoutCh := time.After(time.Duration(r.blockWaitingSeconds) * time.Second)
	// 轮询 ticker，每隔 50 ms 尝试取锁一次
	ticker := time.NewTicker(time.Duration(50) * time.Millisecond)
	defer ticker.Stop()
	
	for range ticker.C {
		select {
		// ctx 终止了
		case <-ctx.Done():
			return fmt.Errorf("lock failed, ctx timeout, err: %w", ctx.Err())
			// 阻塞等锁达到上限时间
		case <-timeoutCh:
			return fmt.Errorf("block waiting time out, err: %w", ErrLockAcquiredByOthers)
		// 放行
		default:
		}
		
		// 尝试取锁
		err := r.tryLock(ctx)
		if err == nil {
			// 加锁成功，返回结果
			return nil
		}
		
		// 不可重试类型的错误，直接返回
		if !IsRetryableErr(err) {
			return err
		}
	}
	
	return nil
}
```
# 解锁
基于 lua 脚本原子化
1. `get`操作, 校验当前操作者是否拥有锁的所有权
2. 倘若是，执行`del`删除锁数据, 释放锁
```go
// Unlock 解锁. 基于 lua 脚本实现操作原子性.
func (r *RedisLock) Unlock(ctx context.Context) (err error) {
	defer func() {
		if err != nil {
			return
		}
		// 停止看门狗
		if r.stopDog != nil {
			r.stopDog()
		}
	}()

	keysAndArgs := []interface{}{r.getLockKey(), r.token}
	reply, _err := r.client.Eval(ctx, LuaCheckAndDeleteDistributionLock, 1, keysAndArgs)
	if _err != nil {
		err = _err
		return
	}

	if ret, _ := reply.(int64); ret != 1 {
		err = errors.New("can not unlock without ownership of lock")
	}

	return nil
}

// LuaCheckAndDeleteDistributionLock 判断是否拥有分布式锁的归属权，是则删除
const LuaCheckAndDeleteDistributionLock = `
  local lockerKey = KEYS[1]
  local targetToken = ARGV[1]
  local getToken = redis.call('get',lockerKey)
  if (not getToken or getToken ~= targetToken) then
    return 0
  else
    return redis.call('del',lockerKey)
  end
`
```
# 延期锁
基于 lua 脚本
1. `get`操作获取 val，查看是否和当前使用方身份一致
2. 如果一致，执行`expire`更新过期时间
```go
// 更新锁的过期时间，基于 lua 脚本实现操作原子性
func (r *RedisLock) DelayExpire(ctx context.Context, expireSeconds int64) error {
  keysAndArgs := []interface{}{r.getLockKey(), r.token, expireSeconds}
  reply, err := r.client.Eval(ctx, LuaCheckAndExpireDistributionLock, 1, keysAndArgs)
  if err != nil {
      return err
  }
  if ret, _ := reply.(int64); ret != 1 {
      return errors.New("can not expire lock without ownership of lock")
  }
  return nil
}
const LuaCheckAndExpireDistributionLock = `
  local lockerKey = KEYS[1]
  local targetToken = ARGV[1]
  local duration = ARGV[2]
  local getToken = redis.call('get',lockerKey)
  if (not getToken or getToken ~= targetToken) then
    return 0
  else
    return redis.call('expire',lockerKey,duration)
  end
`
```
# 看门狗机制
==解决过期时间不精确==
机制类似etcd的`lease`，在加锁成功后，异步启动看门狗(定时器)，定时更新锁的过期时间
*保证同一时刻只会有一只看门狗*
释放锁时，停止看门狗 
```go
// 启动看门狗
func (r *RedisLock) watchDog(ctx context.Context) {
	// 1. 非看门狗模式，不处理
	if !r.watchDogMode {
		return
	}
	
	
	// 2. 确保之前启动的看门狗已经正常回收
	for !atomic.CompareAndSwapInt32(&r.runningDog, 0, 1) {
	}
	

	// 3. 启动看门狗
	ctx, r.stopDog = context.WithCancel(ctx)
	go func() {
		defer func() {
			atomic.StoreInt32(&r.runningDog, 0)
		}()
		r.runWatchDog(ctx)
	}()
}

func (r *RedisLock) runWatchDog(ctx context.Context) {
	ticker := time.NewTicker(WatchDogWorkStepSeconds * time.Second)
	defer ticker.Stop()
	
	
	for range ticker.C {
		select {
		case <-ctx.Done():
			return
		default:
		}
		
		
		// 看门狗负责在用户未显式解锁时，持续为分布式锁进行续期
		// 通过 lua 脚本，延期之前会确保保证锁仍然属于自己
		_ = r.DelayExpire(ctx, WatchDogWorkStepSeconds)
	}
}

```
# 红锁
==解决数据弱一致性==: 一锁多主
基于多数派原则, 需要对多个节点进行加锁, 半数以上成功才算加锁成功
如果失败, 对已加锁的节点全部解锁 
## 分布式锁 Redlock 的争议与思辨：一份总结笔记
### 一、 质疑方 (Martin Kleppmann) 的核心论点：Redlock 不安全
Martin Kleppmann（《数据密集型应用系统设计》作者）认为 Redlock 的安全性严重依赖于对**时间**的假设，这在复杂的分布式环境中是不可靠的，因此 Redlock 不是一个安全的分布式锁实现。
**1. 核心问题：依赖本地时间和时钟**
- **Redis 节点依赖本地时间**：节点根据自己的本地时钟来判断锁是否过期。
- **客户端依赖本地时间**：客户端通过计算获取锁前后的时间差，来判断锁的剩余有效时间。
- **时钟不可靠**：系统时间会发生**时钟漂移**（clocks drift）和**时钟跳变**（clocks jump），例如 NTP 同步或管理员手动修改，这会导致锁的过期行为不符合预期。
2. 关键反例：进程暂停 (Process Pause)
这是一个经典的攻击场景，其中 Stop-The-World GC (垃圾回收) 只是导致进程暂停的一种情况：
3. **客户端1** 成功在多数 Redis 节点上获取了锁。
4. 在操作共享资源**之前**，客户端1 进入了长时间的 GC，进程被暂停。
5. 在客户端1暂停期间，它在 Redis 节点上持有的锁因超时而**自动过期**了。
6. **客户端2** 此时成功获取了同一个分布式锁。
7. 客户端1 的 GC 结束，它恢复运行，但它**并不知道**自己的锁早已过期，于是继续操作共享资源。
8. **结果**：客户端1 和客户端2 同时持有锁，并操作共享资源，**破坏了互斥性**。
9. Martin 的结论：Redlock 的安全性假设过于脆弱
Redlock 的安全建立在三个脆弱的假设之上：
- **有上限的网络延迟 (bounded network delay)**
- **有上限的进程暂停时间 (bounded process pauses)**
- **有上限的时钟漂移 (bounded clock drift)**
Martin 认为，一个真正安全的异步算法，其**安全性 (Safety)** 不应依赖任何时间假设。即使系统时序混乱，算法顶多是性能下降（影响活性 Liveness），但不应做出错误的决策。而 Redlock 恰恰违反了这一点。
### 二、质疑方的解决方案：Fencing Token (防护令牌)
为了解决上述问题，Martin 提出了 **Fencing Token** 机制。
- **工作原理**：
    1. 锁服务在每次授予锁时，同时返回一个**单调递增的 Token**（令牌）。
    2. 客户端在每次访问共享资源时，都必须带上这个 Token。
    3. 共享资源本身需要有判断逻辑：它会记录下最后一次成功操作的 Token，并**拒绝**任何携带旧 (stale) Token 的请求。
- **如何解决进程暂停问题**：
    1. 客户端1 获取锁，得到 Token **33**，然后被 GC 暂停。
    2. 锁过期后，客户端2 获取锁，得到 Token **34**，并成功操作了共享资源。共享资源记录下最新 Token 为 34。
    3. 客户端1 恢复后，带着自己过期的 Token 33 去请求操作资源。
    4. 共享资源发现 `33 < 34`，拒绝了这次请求，从而保证了数据安全。
- **Redlock 的局限**：Redlock 难以实现 Fencing Token，因为它的多个 Redis 节点互相独立、不通信，很难在节点间生成一个全局单调递增的 Token。
### 三、反驳方 (Antirez) 的核心论点：问题普遍存在，Redlock 在现实中可行
Redis 的作者 Antirez 对 Martin 的质疑进行了反驳，认为其批评过于理论化，脱离了现实场景。
**1. 论点一：Fencing Token 并非银弹，且暗示了不同的问题**
- 如果共享资源**有能力检查 Token**（例如通过 `UPDATE ... WHERE token=...`），那说明共享资源本身就具备了原子操作能力。在这种情况下，你可能**根本不需要分布式锁**，直接利用资源本身的并发控制机制即可。
- 人们使用分布式锁，通常正是因为共享资源（如写文件）**不具备**这种复杂的互斥能力。
**2. 论点二：时钟问题是运维问题，而非算法缺陷**
- Redlock 确实假设时钟以相近的速率运行，这在现实世界中是合理的。
- 至于**时钟跳变**，通常源于：
    - **管理员手动修改时间**：这属于严重的人为错误，任何分布式系统都无法抵御这种“破坏”。
    - **NTP 强制同步**：可以通过配置 `ntpd` 服务，使其平滑地调整时间（slewing）而非直接跳变（jumping），来避免这个问题。
- 这应该通过**规范的运维**来解决，而不是归咎于算法。
**3. 论点三：进程暂停等问题是所有分布式锁的共同挑战**
- Antirez 承认，客户端在获取锁后、操作资源前发生暂停，是一个真实存在的问题。
- 但他强调，这**并非 Redlock 独有的缺陷**。他用 Martin 推崇的 **ZooKeeper** 举例，证明了同样的问题依然存在：
    1. 客户端1 用 ZooKeeper 获取了锁（创建了临时节点）。
    2. 客户端1 发生长时间 GC，无法向 ZK 发送心跳。
    3. ZK 的 Session 超时，自动删除了临时节点，锁被释放。
    4. 客户端2 成功获取了锁。
    5. 客户端1 恢复，认为自己仍持有锁。
    6. **结果**：同样出现了两个客户端同时持有锁的情况。
- Antirez 的核心意思是，这种因客户端长时间暂停导致的锁失效问题，是任何基于租期（lease）的分布式系统（无论是 Redis 的 TTL 还是 ZK 的 Session）的**固有挑战**，而非 Redlock 的设计缺陷。
### **四、最终结论**
这场争论没有绝对的赢家，双方都揭示了分布式系统设计的深刻道理：
- **理论 vs. 现实**：Martin Kleppmann 从**理论安全性**出发，要求算法在最极端（异步、时序混乱）的模型下依然保证正确。Antirez 则从**工程实践**出发，认为在合理的运维和现实假设下，Redlock 是足够安全的。
- **共识问题**：实现一个完美的分布式锁，本质上等同于解决**分布式共识**问题。没有一个简单的分布式锁服务能在所有极端情况下同时完美保证安全性 (Safety) 和活性 (Liveness)。
- **没有银弹**：选择哪种分布式锁，取决于你的业务场景、对风险的容忍度以及你的运维能力。理解其背后的原理和潜在风险，比盲目相信任何一种方案都更加重要。
# redsync
redsync分布式锁原理：
- 如何**解决可重试**问题：利用信号量和PubSub功能实现等待、唤醒，获取锁失败的重试机制。
- 如何**解决超时续约**问题：利用watchDog，每隔一段时间（releaseTime / 3），重置超时时间。
- 如何**解决主从一致性**问题：利用redsync的multiLock，多个独立的Redis节点，必须在所有节点都获取重入锁，才算获取锁成功。其缺陷：运维成本高、实现复杂。